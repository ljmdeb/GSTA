{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing a Black-box model by a Global Single Tree Approximation\n",
    "#A regression case\n",
    "#Laurent Deborde 2019 march 23th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we ll try the global single tree approximation approach in a regression case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import random as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's load the dataset (slightly modified to include the variables names as columns names in a single file)\n",
    "d=pd.read_csv('winequality-white.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4898 entries, 0 to 4897\n",
      "Data columns (total 12 columns):\n",
      "fixed acidity           4898 non-null float64\n",
      "volatile acidity        4898 non-null float64\n",
      "citric acid             4898 non-null float64\n",
      "residual sugar          4898 non-null float64\n",
      "chlorides               4898 non-null float64\n",
      "free sulfur dioxide     4898 non-null float64\n",
      "total sulfur dioxide    4898 non-null float64\n",
      "density                 4898 non-null float64\n",
      "pH                      4898 non-null float64\n",
      "sulphates               4898 non-null float64\n",
      "alcohol                 4898 non-null float64\n",
      "quality                 4898 non-null int64\n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 459.3 KB\n"
     ]
    }
   ],
   "source": [
    "d.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.854788</td>\n",
       "      <td>0.278241</td>\n",
       "      <td>0.334192</td>\n",
       "      <td>6.391415</td>\n",
       "      <td>0.045772</td>\n",
       "      <td>35.308085</td>\n",
       "      <td>138.360657</td>\n",
       "      <td>0.994027</td>\n",
       "      <td>3.188267</td>\n",
       "      <td>0.489847</td>\n",
       "      <td>10.514267</td>\n",
       "      <td>5.877909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.843868</td>\n",
       "      <td>0.100795</td>\n",
       "      <td>0.121020</td>\n",
       "      <td>5.072058</td>\n",
       "      <td>0.021848</td>\n",
       "      <td>17.007137</td>\n",
       "      <td>42.498065</td>\n",
       "      <td>0.002991</td>\n",
       "      <td>0.151001</td>\n",
       "      <td>0.114126</td>\n",
       "      <td>1.230621</td>\n",
       "      <td>0.885639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.800000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.987110</td>\n",
       "      <td>2.720000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.300000</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>0.991723</td>\n",
       "      <td>3.090000</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.800000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>0.043000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>0.993740</td>\n",
       "      <td>3.180000</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>10.400000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.300000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>0.996100</td>\n",
       "      <td>3.280000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>11.400000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14.200000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>1.660000</td>\n",
       "      <td>65.800000</td>\n",
       "      <td>0.346000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>1.038980</td>\n",
       "      <td>3.820000</td>\n",
       "      <td>1.080000</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "count    4898.000000       4898.000000  4898.000000     4898.000000   \n",
       "mean        6.854788          0.278241     0.334192        6.391415   \n",
       "std         0.843868          0.100795     0.121020        5.072058   \n",
       "min         3.800000          0.080000     0.000000        0.600000   \n",
       "25%         6.300000          0.210000     0.270000        1.700000   \n",
       "50%         6.800000          0.260000     0.320000        5.200000   \n",
       "75%         7.300000          0.320000     0.390000        9.900000   \n",
       "max        14.200000          1.100000     1.660000       65.800000   \n",
       "\n",
       "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
       "count  4898.000000          4898.000000           4898.000000  4898.000000   \n",
       "mean      0.045772            35.308085            138.360657     0.994027   \n",
       "std       0.021848            17.007137             42.498065     0.002991   \n",
       "min       0.009000             2.000000              9.000000     0.987110   \n",
       "25%       0.036000            23.000000            108.000000     0.991723   \n",
       "50%       0.043000            34.000000            134.000000     0.993740   \n",
       "75%       0.050000            46.000000            167.000000     0.996100   \n",
       "max       0.346000           289.000000            440.000000     1.038980   \n",
       "\n",
       "                pH    sulphates      alcohol      quality  \n",
       "count  4898.000000  4898.000000  4898.000000  4898.000000  \n",
       "mean      3.188267     0.489847    10.514267     5.877909  \n",
       "std       0.151001     0.114126     1.230621     0.885639  \n",
       "min       2.720000     0.220000     8.000000     3.000000  \n",
       "25%       3.090000     0.410000     9.500000     5.000000  \n",
       "50%       3.180000     0.470000    10.400000     6.000000  \n",
       "75%       3.280000     0.550000    11.400000     6.000000  \n",
       "max       3.820000     1.080000    14.200000     9.000000  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are want to illustrate the validity of the tree approxomatio approach, not \"solve\" the vino verde problem. So for the sake of simplicity we will suppress NaN data\n",
    "d1=d.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1d8f121ac18>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAEKlJREFUeJzt3X+s3XV9x/HnS+omFJUqcoOFrSxpjMxmDG+AzYRcxsZPI7qEBMKkMJeaBY1uTVxdsrDpzFgi2yJxJJ121AxpmEpopBGbbnfOP1CoogXR0GGFUkZ1ZXUFMq1774/7rbnWlp6ee+45957P85GcnHM+9/P9fj/ve865r/P9nO/33FQVkqT2vGzUA5AkjYYBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWrUklEP4KWceuqptWLFir6Xf/7551m6dOngBjQi41IHWMtCNS61jEsdMLdatm/f/oOqet2x+i3oAFixYgUPPfRQ38tPT08zNTU1uAGNyLjUAdayUI1LLeNSB8ytliTf66WfU0CS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktSoBX0msLSQ7Xh6Pzesu2/o2911y5VD36bGk3sAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRxwyAJGcm+dckjyV5NMn7uvbXJNma5PHuelnXniQfS7IzyTeTnDtrXau7/o8nWT1/ZUmSjqWXPYCDwNqqeiNwAXBTkrOBdcC2qloJbOvuA1wOrOwua4DbYSYwgJuB84HzgJsPhYYkafiOGQBV9UxVfa27/T/AY8By4CpgY9dtI/D27vZVwKdqxgPAKUlOBy4FtlbVvqp6DtgKXDbQaiRJPUtV9d45WQF8CXgT8GRVnTLrZ89V1bIknwduqaovd+3bgD8BpoBXVNVfdu1/BrxYVR89bBtrmNlzYGJi4s2bNm3qu7gDBw5w8skn9738QjEudcB41bJ3336efXH42121/NUDX+e4PC7jUgfMrZaLLrpoe1VNHqtfz/8SMsnJwGeB91fVD5MctesR2uol2n+2oWo9sB5gcnKypqameh3iz5menmYuyy8U41IHjFctt915L7fuGP5/Vd113dTA1zkuj8u41AHDqaWno4CSvJyZP/53VtXnuuZnu6kduuu9Xftu4MxZi58B7HmJdknSCPRyFFCATwKPVdXfzPrRZuDQkTyrgXtntV/fHQ10AbC/qp4B7gcuSbKs+/D3kq5NkjQCvey/vgV4J7AjycNd258CtwB3J3kX8CRwdfezLcAVwE7gBeBGgKral+TDwINdvw9V1b6BVCFJOm7HDIDuw9yjTfhffIT+Bdx0lHVtADYczwAlSfPDM4ElqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqOOGQBJNiTZm+SRWW1/nuTpJA93lytm/eyDSXYm+U6SS2e1X9a17UyybvClSJKORy97AHcAlx2h/W+r6pzusgUgydnANcCvdsv8fZITkpwAfBy4HDgbuLbrK0kakSXH6lBVX0qyosf1XQVsqqr/Bb6bZCdwXveznVX1BECSTV3fbx33iCVJA3HMAHgJ70lyPfAQsLaqngOWAw/M6rO7awN46rD284+00iRrgDUAExMTTE9P9z3AAwcOzGn5hWJc6oDxqmXiRFi76uDQtzsfv79xeVzGpQ4YTi39BsDtwIeB6q5vBX4fyBH6Fkeeaqojrbiq1gPrASYnJ2tqaqrPIc68UOay/EIxLnXAeNVy2533cuuOubyH6s+u66YGvs5xeVzGpQ4YTi19PXur6tlDt5P8A/D57u5u4MxZXc8A9nS3j9YuSRqBvg4DTXL6rLvvAA4dIbQZuCbJLyY5C1gJfBV4EFiZ5Kwkv8DMB8Wb+x+2JGmujrkHkOQuYAo4Nclu4GZgKsk5zEzj7ALeDVBVjya5m5kPdw8CN1XVT7r1vAe4HzgB2FBVjw68GklSz3o5CujaIzR/8iX6fwT4yBHatwBbjmt0WjRWrLuvp35rVx3khh779mLXLVcObF1SazwTWJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ16pgBkGRDkr1JHpnV9pokW5M83l0v69qT5GNJdib5ZpJzZy2zuuv/eJLV81OOJKlXvewB3AFcdljbOmBbVa0EtnX3AS4HVnaXNcDtMBMYwM3A+cB5wM2HQkOSNBrHDICq+hKw77Dmq4CN3e2NwNtntX+qZjwAnJLkdOBSYGtV7auq54Ct/HyoSJKGqN/PACaq6hmA7vq0rn058NSsfru7tqO1S5JGZMmA15cjtNVLtP/8CpI1zEwfMTExwfT0dN+DOXDgwJyWXygWQx1rVx3sqd/Eib337cUofy+DrqVX81HzYniO9WJc6oDh1NJvADyb5PSqeqab4tnbte8GzpzV7wxgT9c+dVj79JFWXFXrgfUAk5OTNTU1daRuPZmenmYuyy8Ui6GOG9bd11O/tasOcuuOwb3v2HXd1MDWdbxuu/PegdbSq/moeTE8x3oxLnXAcGrpdwpoM3DoSJ7VwL2z2q/vjga6ANjfTRHdD1ySZFn34e8lXZskaUSO+fYlyV3MvHs/NcluZo7muQW4O8m7gCeBq7vuW4ArgJ3AC8CNAFW1L8mHgQe7fh+qqsM/WJYkDdExA6Cqrj3Kjy4+Qt8CbjrKejYAG45rdJKkeeOZwJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRwz+NUdKcrOjxrOvjsXbVwZ7O5t51y5UD37ZGxz0ASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaNacASLIryY4kDyd5qGt7TZKtSR7vrpd17UnysSQ7k3wzybmDKECS1J9B7AFcVFXnVNVkd38dsK2qVgLbuvsAlwMru8sa4PYBbFuS1Kf5mAK6CtjY3d4IvH1W+6dqxgPAKUlOn4ftS5J6MNcAKOCLSbYnWdO1TVTVMwDd9Wld+3LgqVnL7u7aJEkjkKrqf+Hk9VW1J8lpwFbgvcDmqjplVp/nqmpZkvuAv6qqL3ft24APVNX2w9a5hpkpIiYmJt68adOmvsd34MABTj755L6XXygWQx07nt7fU7+JE+HZFwe33VXLXz24lR2nvfv2D7SWUer1cRnl77sXi+G10qu51HLRRRdtnzUtf1RL+lp7p6r2dNd7k9wDnAc8m+T0qnqmm+LZ23XfDZw5a/EzgD1HWOd6YD3A5ORkTU1N9T2+6elp5rL8QrEY6rhh3X099Vu76iC37pjT0+5n7LpuamDrOl633XnvQGsZpV4fl1H+vnuxGF4rvRpGLX1PASVZmuSVh24DlwCPAJuB1V231cC93e3NwPXd0UAXAPsPTRVJkoZvLm9fJoB7khxaz6er6gtJHgTuTvIu4Eng6q7/FuAKYCfwAnDjHLYtSZqjvgOgqp4Afu0I7f8FXHyE9gJu6nd7kqTB8kxgSWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWrUklEPQJKOZcW6+3rqt3bVQW7osW+vdt1y5UDXt5AYAPOg1ydrr47nST3OT1ZJg+UUkCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1KihB0CSy5J8J8nOJOuGvX1J0oyhBkCSE4CPA5cDZwPXJjl7mGOQJM0Y9reBngfsrKonAJJsAq4CvjUfG9vx9P6BfzWsJI2LYQfAcuCpWfd3A+cPeQyS1LNBf717r+64bOm8byNVNe8b+enGkquBS6vqD7r77wTOq6r3zuqzBljT3X0D8J05bPJU4AdzWH6hGJc6wFoWqnGpZVzqgLnV8stV9bpjdRr2HsBu4MxZ988A9szuUFXrgfWD2FiSh6pqchDrGqVxqQOsZaEal1rGpQ4YTi3DPgroQWBlkrOS/AJwDbB5yGOQJDHkPYCqOpjkPcD9wAnAhqp6dJhjkCTNGPr/BK6qLcCWIW1uIFNJC8C41AHWslCNSy3jUgcMoZahfggsSVo4/CoISWrU2AVAklck+WqSbyR5NMlfjHpMc5XkhCRfT/L5UY9lLpLsSrIjycNJHhr1ePqV5JQkn0ny7SSPJfmNUY+pH0ne0D0Why4/TPL+UY+rX0n+qHvNP5LkriSvGPWY+pHkfV0Nj8734zF2U0BJAiytqgNJXg58GXhfVT0w4qH1LckfA5PAq6rqraMeT7+S7AImq2pRH6edZCPw71X1ie5otpOq6r9HPa656L6m5Wng/Kr63qjHc7ySLGfmtX52Vb2Y5G5gS1XdMdqRHZ8kbwI2MfOtCT8CvgD8YVU9Ph/bG7s9gJpxoLv78u6yaFMuyRnAlcAnRj0WQZJXARcCnwSoqh8t9j/+nYuB/1iMf/xnWQKcmGQJcBKHnWO0SLwReKCqXqiqg8C/Ae+Yr42NXQDAT6dMHgb2Alur6iujHtMc/B3wAeD/Rj2QASjgi0m2d2d8L0a/Anwf+MduWu4TSeb/nP35dw1w16gH0a+qehr4KPAk8Aywv6q+ONpR9eUR4MIkr01yEnAFP3vy7ECNZQBU1U+q6hxmzjQ+r9utWnSSvBXYW1XbRz2WAXlLVZ3LzLfB3pTkwlEPqA9LgHOB26vq14HngUX9tebdNNbbgH8e9Vj6lWQZM18seRbwemBpkt8b7aiOX1U9Bvw1sJWZ6Z9vAAfna3tjGQCHdLvm08BlIx5Kv94CvK2bO98E/FaSfxrtkPpXVXu6673APczMcy42u4Hds/YqP8NMICxmlwNfq6pnRz2QOfht4LtV9f2q+jHwOeA3RzymvlTVJ6vq3Kq6ENgHzMv8P4xhACR5XZJTutsnMvPE+PZoR9WfqvpgVZ1RVSuY2UX/l6padO9qAJIsTfLKQ7eBS5jZ3V1Uquo/gaeSvKFruph5+jrzIbqWRTz903kSuCDJSd2BIBcDj414TH1Jclp3/UvA7zKPj83QzwQegtOBjd1RDS8D7q6qRX345JiYAO6ZeW2yBPh0VX1htEPq23uBO7upkyeAG0c8nr5188y/A7x71GOZi6r6SpLPAF9jZsrk6yzes4I/m+S1wI+Bm6rqufna0NgdBipJ6s3YTQFJknpjAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1Kj/B/dbvMh/I71EAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d8f2755ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d1['quality'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's separate between train and test\n",
    "y=d1['quality']\n",
    "X=d1.drop('quality',axis=1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's first try a single tree on the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'max_depth': range(1,15)}\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best'),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'max_depth': range(1, 15)}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search = GridSearchCV(DecisionTreeRegressor(), param_grid, cv=5,\n",
    "                          return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': 6}\n",
      "Best cross-validation score: 0.28\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set score: 0.29\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set score: {:.2f}\".format(grid_search.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for a depth of 6\n",
      "R2 on training set is 0.42\n",
      "R2 on test set is 0.31\n"
     ]
    }
   ],
   "source": [
    "### fiting with those parameters on the whole training set\n",
    "k=6\n",
    "tree = DecisionTreeRegressor(max_depth=k)\n",
    "tree.fit(X_train,y_train)\n",
    "R2_appr=tree.score(X_train,y_train)\n",
    "R2_test=tree.score(X_test,y_test)\n",
    "print('for a depth of',k)\n",
    "print(\"R2 on training set is {:.2f}\".format(R2_appr))\n",
    "print(\"R2 on test set is {:.2f}\".format(R2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d8f2713d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d8f23fbdd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# adapted from Muller & Guido \n",
    "# https://github.com/amueller/introduction_to_ml_with_python/blob/master/02-supervised-learning.ipynb\n",
    "\n",
    "def plot_feature_importances(model, Frame):\n",
    "    plt.clf()\n",
    "    n_features=len(Frame.columns)\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(np.arange(n_features), Frame.columns)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.ylim(-1, n_features)\n",
    "\n",
    "\n",
    "plot_feature_importances(tree,X_train)\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try a Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': None, 'max_features': 3}\n",
      "Best cross-validation score: 0.44\n",
      "Test set score: 0.51\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'max_depth': [1,2,3,4,5,6, 7,8, 9, 10, 11, 12, None], 'max_features': [3, 5,10,None]}\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "grid_search = GridSearchCV(RandomForestRegressor(), param_grid, cv=5,return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "print(\"Test set score: {:.2f}\".format(grid_search.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 out of sample 0.50\n"
     ]
    }
   ],
   "source": [
    "rdf = RandomForestRegressor(max_depth=None, max_features= 3)\n",
    "rdf.fit(X_train,y_train)\n",
    "print(\"R2 out of sample {:.2f}\".format(rdf.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try a Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with gridSearch (wich is a bit long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'learning_rate': 0.05, 'max_depth': None, 'max_features': 3}\n",
      "Best cross-validation score: 0.51\n",
      "Test set score: 0.56\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'max_depth': [1,2,3,5,None],'max_features': [3,5,10,None],'learning_rate' : [0.03,0.05,0.1,0.2]}\n",
    "grid_search = GridSearchCV(GradientBoostingRegressor(), param_grid, cv=5, return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "print(\"Test set score: {:.2f}\".format(grid_search.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fit on the whole training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le R2 in sample est de 1.00\n",
      "et le R2 out of sample est de 0.56\n"
     ]
    }
   ],
   "source": [
    "gbr = GradientBoostingRegressor(max_depth =None,max_features=3,learning_rate=0.05)\n",
    "gbr.fit(X_train,y_train)\n",
    "print(\"le R2 in sample est de {:.2f}\".format(gbr.score(X_train,y_train)))\n",
    "print(\"et le R2 out of sample est de {:.2f}\".format(gbr.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The RF and GB are much better than the single tree at predecting the wine quality. Let's suppose we are not allowed to use those black box models. We still can build a simple yet improved tree, by using the knowledge provided by the black box. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First generate unlabeled data points \"similar\" to those of the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we use a multivariate gaussian distribution estimated on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "echmean=X_train.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's compute covariance on the training set\n",
    "echcov= X_train.cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's now randomly draw points\n",
    "X_virt= np.random.multivariate_normal(echmean, echcov, 100000, check_valid ='warn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 11)\n"
     ]
    }
   ],
   "source": [
    "print(X_virt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.864985</td>\n",
       "      <td>0.279465</td>\n",
       "      <td>0.333350</td>\n",
       "      <td>6.450871</td>\n",
       "      <td>0.045577</td>\n",
       "      <td>35.272664</td>\n",
       "      <td>138.484540</td>\n",
       "      <td>0.994080</td>\n",
       "      <td>3.189608</td>\n",
       "      <td>0.489310</td>\n",
       "      <td>10.503121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.849752</td>\n",
       "      <td>0.101685</td>\n",
       "      <td>0.120118</td>\n",
       "      <td>5.131988</td>\n",
       "      <td>0.021220</td>\n",
       "      <td>16.778241</td>\n",
       "      <td>42.104778</td>\n",
       "      <td>0.003031</td>\n",
       "      <td>0.149258</td>\n",
       "      <td>0.113389</td>\n",
       "      <td>1.233376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.889589</td>\n",
       "      <td>-0.166731</td>\n",
       "      <td>-0.224860</td>\n",
       "      <td>-15.235920</td>\n",
       "      <td>-0.056763</td>\n",
       "      <td>-37.487850</td>\n",
       "      <td>-44.126767</td>\n",
       "      <td>0.979860</td>\n",
       "      <td>2.519409</td>\n",
       "      <td>-0.009918</td>\n",
       "      <td>5.423965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.292101</td>\n",
       "      <td>0.211125</td>\n",
       "      <td>0.251828</td>\n",
       "      <td>2.989215</td>\n",
       "      <td>0.031297</td>\n",
       "      <td>24.042063</td>\n",
       "      <td>109.949742</td>\n",
       "      <td>0.992040</td>\n",
       "      <td>3.089487</td>\n",
       "      <td>0.412999</td>\n",
       "      <td>9.670512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.866787</td>\n",
       "      <td>0.279320</td>\n",
       "      <td>0.333470</td>\n",
       "      <td>6.444367</td>\n",
       "      <td>0.045684</td>\n",
       "      <td>35.282912</td>\n",
       "      <td>138.491812</td>\n",
       "      <td>0.994073</td>\n",
       "      <td>3.189414</td>\n",
       "      <td>0.488819</td>\n",
       "      <td>10.503024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.434753</td>\n",
       "      <td>0.348098</td>\n",
       "      <td>0.414126</td>\n",
       "      <td>9.900989</td>\n",
       "      <td>0.059867</td>\n",
       "      <td>46.541598</td>\n",
       "      <td>166.805674</td>\n",
       "      <td>0.996118</td>\n",
       "      <td>3.290853</td>\n",
       "      <td>0.566075</td>\n",
       "      <td>11.332737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10.668146</td>\n",
       "      <td>0.712350</td>\n",
       "      <td>0.883823</td>\n",
       "      <td>28.425951</td>\n",
       "      <td>0.137364</td>\n",
       "      <td>111.722872</td>\n",
       "      <td>328.219216</td>\n",
       "      <td>1.007085</td>\n",
       "      <td>3.844140</td>\n",
       "      <td>0.973187</td>\n",
       "      <td>16.521344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed acidity  volatile acidity    citric acid  residual sugar  \\\n",
       "count  100000.000000     100000.000000  100000.000000   100000.000000   \n",
       "mean        6.864985          0.279465       0.333350        6.450871   \n",
       "std         0.849752          0.101685       0.120118        5.131988   \n",
       "min         2.889589         -0.166731      -0.224860      -15.235920   \n",
       "25%         6.292101          0.211125       0.251828        2.989215   \n",
       "50%         6.866787          0.279320       0.333470        6.444367   \n",
       "75%         7.434753          0.348098       0.414126        9.900989   \n",
       "max        10.668146          0.712350       0.883823       28.425951   \n",
       "\n",
       "           chlorides  free sulfur dioxide  total sulfur dioxide  \\\n",
       "count  100000.000000        100000.000000         100000.000000   \n",
       "mean        0.045577            35.272664            138.484540   \n",
       "std         0.021220            16.778241             42.104778   \n",
       "min        -0.056763           -37.487850            -44.126767   \n",
       "25%         0.031297            24.042063            109.949742   \n",
       "50%         0.045684            35.282912            138.491812   \n",
       "75%         0.059867            46.541598            166.805674   \n",
       "max         0.137364           111.722872            328.219216   \n",
       "\n",
       "             density             pH      sulphates        alcohol  \n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000  \n",
       "mean        0.994080       3.189608       0.489310      10.503121  \n",
       "std         0.003031       0.149258       0.113389       1.233376  \n",
       "min         0.979860       2.519409      -0.009918       5.423965  \n",
       "25%         0.992040       3.089487       0.412999       9.670512  \n",
       "50%         0.994073       3.189414       0.488819      10.503024  \n",
       "75%         0.996118       3.290853       0.566075      11.332737  \n",
       "max         1.007085       3.844140       0.973187      16.521344  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_virt=pd.DataFrame(X_virt, columns=X_train.columns)\n",
    "d_virt.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is the trick : let's label those new data using the black box model as an oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_virt=gbr.predict(d_virt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.33091855 5.94922479 5.95162938 ... 6.4448302  6.52707485 6.02740273]\n"
     ]
    }
   ],
   "source": [
    "print(y_virt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's try a single tree on those numerous virtual datas (we don't bother to add the true training set since it's so small compared to the virtual one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 4 average cross-validation score: 0.63\n",
      "k= 5 average cross-validation score: 0.69\n",
      "k= 6 average cross-validation score: 0.74\n",
      "k= 7 average cross-validation score: 0.76\n",
      "k= 8 average cross-validation score: 0.79\n",
      "k= 9 average cross-validation score: 0.80\n",
      "k= 10 average cross-validation score: 0.82\n",
      "k= 11 average cross-validation score: 0.83\n",
      "k= 12 average cross-validation score: 0.83\n",
      "k= 13 average cross-validation score: 0.83\n",
      "k= 14 average cross-validation score: 0.83\n"
     ]
    }
   ],
   "source": [
    "for i in range (4,15):\n",
    "    tree = DecisionTreeRegressor(max_depth=i,random_state=42)\n",
    "    score=cross_val_score(tree,d_virt,y_virt,cv=5).mean() \n",
    "    print(\"k=\",i,\"average cross-validation score: {:.2f}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The trees fit quite well the virtual data. But the important question is : do they fit the true training set ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pour une profondeur de 11\n",
      "le R2 sur l'chantillon d'apprentissage original est de 0.43\n",
      "et le R2 sur le vrai chantillon de test est de 0.37\n"
     ]
    }
   ],
   "source": [
    "# Optimum seems ton be around 11 but that makes a quite complicated tree. Let's limit it to 8\n",
    "k=11\n",
    "tree = DecisionTreeRegressor(max_depth=k)\n",
    "tree.fit(d_virt,y_virt)\n",
    "R2_appr=tree.score(X_train,y_train)\n",
    "R2_test=tree.score(X_test,y_test)\n",
    "print('pour une profondeur de',k)\n",
    "print(\"le R2 sur l'chantillon d'apprentissage original est de {:.2f}\".format(R2_appr))\n",
    "print(\"et le R2 sur le vrai chantillon de test est de {:.2f}\".format(R2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### better than the initial tree, but much lower thant the best models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# essayons avec les prvisions du random forest\n",
    "z_virt=rdf.predict(d_virt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 5 average cross-validation score: 0.58\n",
      "k= 6 average cross-validation score: 0.62\n",
      "k= 7 average cross-validation score: 0.65\n",
      "k= 8 average cross-validation score: 0.67\n",
      "k= 9 average cross-validation score: 0.69\n",
      "k= 10 average cross-validation score: 0.70\n",
      "k= 11 average cross-validation score: 0.70\n"
     ]
    }
   ],
   "source": [
    "for i in range (5,12):\n",
    "    tree1 = DecisionTreeRegressor(max_depth=i,random_state=42)\n",
    "    score=cross_val_score(tree1,d_virt,z_virt,cv=5).mean() \n",
    "    print(\"k=\",i,\"average cross-validation score: {:.2f}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pour une profondeur de 10\n",
      "le R2 sur l'chantillon d'apprentissage original est de 0.41\n",
      "et le R2 sur le vrai chantillon de test est de 0.36\n"
     ]
    }
   ],
   "source": [
    "# #the trees dont fit the artificial data very well. \n",
    "# still let's look at the performance on the test set\n",
    "k=10\n",
    "tree1 = DecisionTreeRegressor(max_depth=k)\n",
    "tree1.fit(d_virt,y_virt)\n",
    "R2_appr=tree1.score(X_train,y_train)\n",
    "R2_test=tree1.score(X_test,y_test)\n",
    "print('pour une profondeur de',k)\n",
    "print(\"le R2 sur l'chantillon d'apprentissage original est de {:.2f}\".format(R2_appr))\n",
    "print(\"et le R2 sur le vrai chantillon de test est de {:.2f}\".format(R2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better than the original tree, but by a thin margin. A larger number of additionnal data would probably help, but will lengthen the computing time. Let's try this with a determinist strategy to generate additionnal data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Rather than gaussian random draws, we will use a deterministic procedure : take the middle point between each pair of initial data (that will be a huge number of points in that case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.349093e+07</td>\n",
       "      <td>1.349093e+07</td>\n",
       "      <td>1.349093e+07</td>\n",
       "      <td>1.349093e+07</td>\n",
       "      <td>1.349093e+07</td>\n",
       "      <td>1.349093e+07</td>\n",
       "      <td>1.349093e+07</td>\n",
       "      <td>1.349093e+07</td>\n",
       "      <td>1.349093e+07</td>\n",
       "      <td>1.349093e+07</td>\n",
       "      <td>1.349093e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.865369e+00</td>\n",
       "      <td>2.791778e-01</td>\n",
       "      <td>3.332072e-01</td>\n",
       "      <td>6.437912e+00</td>\n",
       "      <td>4.559406e-02</td>\n",
       "      <td>3.518296e+01</td>\n",
       "      <td>1.383041e+02</td>\n",
       "      <td>9.940657e-01</td>\n",
       "      <td>3.189363e+00</td>\n",
       "      <td>4.895426e-01</td>\n",
       "      <td>1.051027e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.987321e-01</td>\n",
       "      <td>7.180040e-02</td>\n",
       "      <td>8.493012e-02</td>\n",
       "      <td>3.634005e+00</td>\n",
       "      <td>1.497541e-02</td>\n",
       "      <td>1.188677e+01</td>\n",
       "      <td>2.971338e+01</td>\n",
       "      <td>2.139934e-03</td>\n",
       "      <td>1.057565e-01</td>\n",
       "      <td>8.029811e-02</td>\n",
       "      <td>8.699087e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.800000e+00</td>\n",
       "      <td>8.000000e-02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.000000e-01</td>\n",
       "      <td>9.000000e-03</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>9.871100e-01</td>\n",
       "      <td>2.720000e+00</td>\n",
       "      <td>2.200000e-01</td>\n",
       "      <td>8.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.450000e+00</td>\n",
       "      <td>2.300000e-01</td>\n",
       "      <td>2.800000e-01</td>\n",
       "      <td>3.625000e+00</td>\n",
       "      <td>3.800000e-02</td>\n",
       "      <td>2.700000e+01</td>\n",
       "      <td>1.175000e+02</td>\n",
       "      <td>9.925400e-01</td>\n",
       "      <td>3.115000e+00</td>\n",
       "      <td>4.350000e-01</td>\n",
       "      <td>9.850000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.850000e+00</td>\n",
       "      <td>2.700000e-01</td>\n",
       "      <td>3.250000e-01</td>\n",
       "      <td>6.100000e+00</td>\n",
       "      <td>4.300000e-02</td>\n",
       "      <td>3.450000e+01</td>\n",
       "      <td>1.375000e+02</td>\n",
       "      <td>9.939900e-01</td>\n",
       "      <td>3.185000e+00</td>\n",
       "      <td>4.800000e-01</td>\n",
       "      <td>1.045000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.250000e+00</td>\n",
       "      <td>3.150000e-01</td>\n",
       "      <td>3.800000e-01</td>\n",
       "      <td>8.775000e+00</td>\n",
       "      <td>4.850000e-02</td>\n",
       "      <td>4.250000e+01</td>\n",
       "      <td>1.580000e+02</td>\n",
       "      <td>9.954650e-01</td>\n",
       "      <td>3.255000e+00</td>\n",
       "      <td>5.350000e-01</td>\n",
       "      <td>1.110000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.180000e+01</td>\n",
       "      <td>1.100000e+00</td>\n",
       "      <td>1.660000e+00</td>\n",
       "      <td>6.580000e+01</td>\n",
       "      <td>3.460000e-01</td>\n",
       "      <td>1.465000e+02</td>\n",
       "      <td>3.130000e+02</td>\n",
       "      <td>1.038980e+00</td>\n",
       "      <td>3.820000e+00</td>\n",
       "      <td>1.060000e+00</td>\n",
       "      <td>1.420000e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed acidity  volatile acidity   citric acid  residual sugar  \\\n",
       "count   1.349093e+07      1.349093e+07  1.349093e+07    1.349093e+07   \n",
       "mean    6.865369e+00      2.791778e-01  3.332072e-01    6.437912e+00   \n",
       "std     5.987321e-01      7.180040e-02  8.493012e-02    3.634005e+00   \n",
       "min     3.800000e+00      8.000000e-02  0.000000e+00    6.000000e-01   \n",
       "25%     6.450000e+00      2.300000e-01  2.800000e-01    3.625000e+00   \n",
       "50%     6.850000e+00      2.700000e-01  3.250000e-01    6.100000e+00   \n",
       "75%     7.250000e+00      3.150000e-01  3.800000e-01    8.775000e+00   \n",
       "max     1.180000e+01      1.100000e+00  1.660000e+00    6.580000e+01   \n",
       "\n",
       "          chlorides  free sulfur dioxide  total sulfur dioxide       density  \\\n",
       "count  1.349093e+07         1.349093e+07          1.349093e+07  1.349093e+07   \n",
       "mean   4.559406e-02         3.518296e+01          1.383041e+02  9.940657e-01   \n",
       "std    1.497541e-02         1.188677e+01          2.971338e+01  2.139934e-03   \n",
       "min    9.000000e-03         3.000000e+00          1.000000e+01  9.871100e-01   \n",
       "25%    3.800000e-02         2.700000e+01          1.175000e+02  9.925400e-01   \n",
       "50%    4.300000e-02         3.450000e+01          1.375000e+02  9.939900e-01   \n",
       "75%    4.850000e-02         4.250000e+01          1.580000e+02  9.954650e-01   \n",
       "max    3.460000e-01         1.465000e+02          3.130000e+02  1.038980e+00   \n",
       "\n",
       "                 pH     sulphates       alcohol  \n",
       "count  1.349093e+07  1.349093e+07  1.349093e+07  \n",
       "mean   3.189363e+00  4.895426e-01  1.051027e+01  \n",
       "std    1.057565e-01  8.029811e-02  8.699087e-01  \n",
       "min    2.720000e+00  2.200000e-01  8.000000e+00  \n",
       "25%    3.115000e+00  4.350000e-01  9.850000e+00  \n",
       "50%    3.185000e+00  4.800000e-01  1.045000e+01  \n",
       "75%    3.255000e+00  5.350000e-01  1.110000e+01  \n",
       "max    3.820000e+00  1.060000e+00  1.420000e+01  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long=len(X_train)\n",
    "court=len(X_train.columns)\n",
    "X_virt=np.zeros((long*long,court))\n",
    "X_val=X_train.values\n",
    "for i in range(long):\n",
    "    for j in range(long):\n",
    "        for k in range(court):\n",
    "            X_virt[i+long*j,k]=(X_val[i,k]+X_val[j,k])/2\n",
    "d_virt=pd.DataFrame(X_virt, columns=X_train.columns)\n",
    "d_virt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's label those points with the black box prediction\n",
    "y_virt=gbr.predict(d_virt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 7 average cross-validation score: 0.807\n",
      "k= 8 average cross-validation score: 0.826\n",
      "k= 9 average cross-validation score: 0.843\n",
      "k= 10 average cross-validation score: 0.858\n",
      "k= 11 average cross-validation score: 0.872\n",
      "k= 12 average cross-validation score: 0.883\n",
      "k= 13 average cross-validation score: 0.893\n",
      "k= 14 average cross-validation score: 0.902\n"
     ]
    }
   ],
   "source": [
    "#as above, let(s fit a tree on those data (it will take time !)\n",
    "for i in range (10,17):\n",
    "    tree = DecisionTreeRegressor(max_depth=i,random_state=42)\n",
    "    score=cross_val_score(tree,d_virt,y_virt,cv=5).mean() \n",
    "    print(\"k=\",i,\"average cross-validation score: {:.3f}\".format(score))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pour une profondeur de 14\n",
      "le R2 sur l'chantillon d'apprentissage original est de 0.63\n",
      "et le R2 sur le vrai chantillon de test est de 0.41\n"
     ]
    }
   ],
   "source": [
    "# Does the optimal tree fits well the real data ? \n",
    "k=14\n",
    "tree = DecisionTreeRegressor(max_depth=k)\n",
    "tree.fit(d_virt,y_virt)\n",
    "R2_appr=tree.score(X_train,y_train)\n",
    "R2_test=tree.score(X_test,y_test)\n",
    "print('pour une profondeur de',k)\n",
    "print(\"le R2 sur l'chantillon d'apprentissage original est de {:.2f}\".format(R2_appr))\n",
    "print(\"et le R2 sur le vrai chantillon de test est de {:.2f}\".format(R2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### We observe a marked improvement over both the initial tree and the tree obtained from a smaller number of artificial data. The  resulting tree is a bit complicated (deep). Is a less deep tree a good predictor ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pour une profondeur de 9\n",
      "le R2 sur l'chantillon d'apprentissage original est de 0.41\n",
      "et le R2 sur le vrai chantillon de test est de 0.35\n"
     ]
    }
   ],
   "source": [
    "k=9\n",
    "tree = DecisionTreeRegressor(max_depth=k)\n",
    "tree.fit(d_virt,y_virt)\n",
    "R2_appr=tree.score(X_train,y_train)\n",
    "R2_test=tree.score(X_test,y_test)\n",
    "print('pour une profondeur de',k)\n",
    "print(\"le R2 sur l'chantillon d'apprentissage original est de {:.2f}\".format(R2_appr))\n",
    "print(\"et le R2 sur le vrai chantillon de test est de {:.2f}\".format(R2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we lower the depth of the tree we markedly lower performance. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
